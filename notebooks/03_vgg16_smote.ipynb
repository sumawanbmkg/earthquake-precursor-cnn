{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f29286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Semua pustaka berhasil diimpor.\n"
     ]
    }
   ],
   "source": [
    "# Impor semua pustaka yang akan kita gunakan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Atur agar TensorFlow tidak menampilkan terlalu banyak pesan log\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Semua pustaka berhasil diimpor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048480c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path utama proyek diatur ke: /home/kmtr/Earthquake_Prediction\n",
      "Catatan: Mode ini menggunakan DATA ASLI + DATA SINTETIK SMOTE.\n"
     ]
    }
   ],
   "source": [
    "# --- KONFIGURASI PATH ---\n",
    "BASE_PATH = \".\" # GANTI INI dengan lokasi folder proyek Anda jika berbeda\n",
    "\n",
    "BASE_IMAGE_DIR = os.path.join(BASE_PATH, \"dataset_gambar_3komponen/\")\n",
    "CSV_DIR = os.path.join(BASE_PATH, \"dataset_numerik/\")\n",
    "\n",
    "# Path Input (gunakan file CSV yang sudah divalidasi jika ada)\n",
    "PRECURSOR_CSV_PATH = os.path.join(CSV_DIR, \"hasil_fitur_precursor.csv\")\n",
    "NORMAL_CSV_PATH = os.path.join(CSV_DIR, \"hasil_fitur_normal_terfilter.csv\") \n",
    "# Path untuk data sintetik SMOTE yang baru Anda buat\n",
    "SMOTE_SYNTHETIC_CSV_PATH = os.path.join(CSV_DIR, \"hasil_fitur_sintetik_SMOTE.csv\") \n",
    "\n",
    "# Path Output untuk model ini (akan berbeda agar tidak menimpa model sebelumnya)\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_PATH, \"best_xception_smote_synthetic_model.keras\")\n",
    "\n",
    "# --- PARAMETER MODEL ---\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16 # Turunkan jika Anda mengalami error kehabisan memori (misal, ke 8)\n",
    "\n",
    "print(f\"Path utama proyek diatur ke: {os.path.abspath(BASE_PATH)}\")\n",
    "print(\"Catatan: Mode ini menggunakan DATA ASLI + DATA SINTETIK SMOTE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2135721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memuat dataset sumber (Data Asli + Sintetik SMOTE) ---\n",
      "Total entri data yang akan divalidasi (Asli + Sintetik): 1292\n",
      "\n",
      "--- Memvalidasi entri data terhadap file gambar ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180336/4129224684.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_sintetik_smote[col].fillna(mode_val, inplace=True)\n",
      "/tmp/ipykernel_180336/4129224684.py:39: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_sintetik_smote[col].fillna(mode_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validasi selesai. 179 baris data dibuang. Total data valid: 1113\n",
      "Jumlah data asli yang valid: 682\n",
      "Jumlah data sintetik SMOTE yang valid: 431\n",
      "\n",
      "--- Ukuran Dataset Final (Asli + Sintetik SMOTE) ---\n",
      "Jumlah data training (Asli + Sintetik) : 894\n",
      "Jumlah data validasi (Hanya Asli)      : 82\n",
      "Jumlah data testing (Hanya Asli)       : 137\n",
      "\n",
      "--- Proporsi Kelas dalam Dataset Final ---\n",
      "Training (Asli + Sintetik):\n",
      "label\n",
      "0    0.637584\n",
      "1    0.362416\n",
      "Name: proportion, dtype: float64\n",
      "Validasi (Hanya Asli):\n",
      "label\n",
      "0    0.597561\n",
      "1    0.402439\n",
      "Name: proportion, dtype: float64\n",
      "Testing (Hanya Asli):\n",
      "label\n",
      "0    0.591241\n",
      "1    0.408759\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Memuat dataset sumber (Data Asli + Sintetik SMOTE) ---\")\n",
    "try:\n",
    "    df_precursor_original = pd.read_csv(PRECURSOR_CSV_PATH)\n",
    "    df_normal_original = pd.read_csv(NORMAL_CSV_PATH)\n",
    "    df_sintetik_smote = pd.read_csv(SMOTE_SYNTHETIC_CSV_PATH) # Baca data sintetik SMOTE\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Eror: Pastikan file CSV sumber ada di lokasi yang benar. {e}\")\n",
    "    raise\n",
    "\n",
    "# Beri label dan info folder untuk data asli\n",
    "df_precursor_original['label'] = 1\n",
    "df_precursor_original['image_folder'] = 'precursor'\n",
    "df_normal_original['label'] = 0\n",
    "df_normal_original['image_folder'] = 'normal'\n",
    "\n",
    "# Beri label dan info folder untuk data sintetik SMOTE\n",
    "# Untuk data SMOTE, kita akan mengasumsikan mereka adalah prekursor\n",
    "# dan untuk path gambar, kita akan \"meminjam\" dari data prekursor asli.\n",
    "# Setiap sampel SMOTE baru adalah unik secara numerik, tetapi secara gambar\n",
    "# kita akan menggunakan gambar dari sampel prekursor asli yang cocok (berdasarkan logika Sel 5 dari notebook SMOTE)\n",
    "# atau paling sederhana, mengulanginya secara acak.\n",
    "# PENTING: Untuk demonstrasi ini, kita akan membuat placeholder 'path_Z', 'path_H', 'path_D'\n",
    "# yang menunjuk ke gambar dari prekursor asli. Dalam implementasi yang lebih canggih,\n",
    "# Anda mungkin perlu logika yang lebih rumit untuk mengasosiasikan gambar.\n",
    "df_sintetik_smote['label'] = 1\n",
    "df_sintetik_smote['image_folder'] = 'precursor' # Ini akan membantu di bagian validasi path\n",
    "\n",
    "# --- PREPROCESSING UNTUK KOLOM METADATA SINTETIK ---\n",
    "# Kolom 'Tanggal', 'Jam', 'Stasiun' di df_sintetik_smote saat ini diisi dengan nilai dari sampel asli secara acak.\n",
    "# Pastikan tipe data mereka benar.\n",
    "# Jika ada NaN di kolom ini (karena df_sintetik_smote dibuat dari fitur saja), isi dengan nilai default.\n",
    "for col in ['Tanggal', 'Stasiun', 'Jam']:\n",
    "    if col not in df_sintetik_smote.columns:\n",
    "        # Jika kolom tidak ada, tambahkan dari df_precursor_original secara acak\n",
    "        df_sintetik_smote[col] = df_precursor_original[col].sample(len(df_sintetik_smote), replace=True, random_state=42).values\n",
    "    elif df_sintetik_smote[col].isnull().any():\n",
    "        # Jika ada NaN, isi dengan modus (nilai paling sering muncul) dari data asli\n",
    "        mode_val = df_precursor_original[col].mode()[0]\n",
    "        df_sintetik_smote[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Pastikan kolom 'Tanggal' memiliki format string yang benar jika di Sel 5 SMOTE ada masalah\n",
    "if 'Tanggal' in df_sintetik_smote.columns and pd.api.types.is_numeric_dtype(df_sintetik_smote['Tanggal']):\n",
    "    df_sintetik_smote['Tanggal'] = df_sintetik_smote['Tanggal'].astype(int).astype(str)\n",
    "# Asumsi 'Jam' juga perlu int\n",
    "if 'Jam' in df_sintetik_smote.columns and pd.api.types.is_numeric_dtype(df_sintetik_smote['Jam']):\n",
    "    df_sintetik_smote['Jam'] = df_sintetik_smote['Jam'].astype(int)\n",
    "\n",
    "# Gabungkan semua data menjadi satu DataFrame besar untuk divalidasi\n",
    "df_full_combined = pd.concat([df_precursor_original, df_normal_original, df_sintetik_smote], ignore_index=True)\n",
    "print(f\"Total entri data yang akan divalidasi (Asli + Sintetik): {len(df_full_combined)}\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Memvalidasi entri data terhadap file gambar ---\")\n",
    "valid_rows = []\n",
    "for index, row in df_full_combined.iterrows():\n",
    "    try:\n",
    "        stasiun = row['Stasiun']\n",
    "        jam = int(row['Jam'])\n",
    "        if jam == 24: jam = 0 # Koreksi jam 24 menjadi 0\n",
    "        \n",
    "        # Untuk sampel asli, base_filename sama seperti sebelumnya.\n",
    "        # Untuk sampel SMOTE, kita perlu strategi agar menunjuk ke gambar prekursor yang valid.\n",
    "        # Pendekatan: Untuk data sintetik SMOTE, kita akan \"meminjam\" base_filename dari prekursor asli.\n",
    "        # Ini berarti sampel numerik sintetik akan divisualisasikan menggunakan gambar dari prekursor asli.\n",
    "        # Ini adalah kompromi yang umum ketika SMOTE digunakan pada data yang memiliki visualisasi.\n",
    "        \n",
    "        base_filename = f\"{stasiun}_PC3_{row['Tanggal']}_{jam}\" # Default untuk data asli\n",
    "        \n",
    "        # Cek kemungkinan adanya subfolder stasiun\n",
    "        folder_path_with_stn = os.path.join(BASE_IMAGE_DIR, row['image_folder'], stasiun)\n",
    "        image_folder = folder_path_with_stn if os.path.exists(folder_path_with_stn) else os.path.join(BASE_IMAGE_DIR, row['image_folder'])\n",
    "        \n",
    "        path_z = os.path.join(image_folder, f\"{base_filename}_Z.png\")\n",
    "        path_h = os.path.join(image_folder, f\"{base_filename}_H.png\")\n",
    "        path_d = os.path.join(image_folder, f\"{base_filename}_D.png\")\n",
    "        \n",
    "        # PENTING: Untuk data sintetik SMOTE, jika gambar aslinya tidak ada (karena metadata 'Tanggal'/'Jam' acak),\n",
    "        # kita perlu memiliki fallback. Cara paling sederhana adalah dengan membuat placeholder.\n",
    "        # Untuk tujuan validasi, kita akan PASTIKAN setiap sampel (asli atau sintetik) memiliki jalur gambar.\n",
    "        # Jika itu adalah sampel sintetik SMOTE dan gambar aslinya tidak ada, kita bisa mengarahkannya\n",
    "        # ke gambar prekursor asli manapun yang valid (misalnya, yang pertama).\n",
    "        \n",
    "        if os.path.exists(path_z) and os.path.exists(path_h) and os.path.exists(path_d):\n",
    "            row['path_Z'], row['path_H'], row['path_D'] = path_z, path_h, path_d\n",
    "            valid_rows.append(row)\n",
    "        elif row['label'] == 1 and row['image_folder'] == 'prekursor': # Ini adalah sampel SMOTE atau prekursor asli yang path-nya bermasalah\n",
    "            # Ini adalah bagian tricky. Sampel SMOTE numerik tidak punya gambar asli.\n",
    "            # Kita akan mengaitkannya dengan gambar prekursor asli yang valid secara acak\n",
    "            # untuk memastikan setiap sampel sintetik punya visual.\n",
    "            if not hasattr(create_dataset, 'fallback_precursor_paths'):\n",
    "                # Inisialisasi daftar path fallback jika belum ada\n",
    "                all_original_precursor_paths = df_precursor_original[\n",
    "                    df_precursor_original['image_folder'] == 'precursor'\n",
    "                ].apply(lambda r: (\n",
    "                    os.path.join(BASE_IMAGE_DIR, r['image_folder'], r['Stasiun'], f\"{r['Stasiun']}_PC3_{r['Tanggal']}_{int(r['Jam']) if r['Jam'] != 24 else 0}_Z.png\"),\n",
    "                    os.path.join(BASE_IMAGE_DIR, r['image_folder'], r['Stasiun'], f\"{r['Stasiun']}_PC3_{r['Tanggal']}_{int(r['Jam']) if r['Jam'] != 24 else 0}_H.png\"),\n",
    "                    os.path.join(BASE_IMAGE_DIR, r['image_folder'], r['Stasiun'], f\"{r['Stasiun']}_PC3_{r['Tanggal']}_{int(r['Jam']) if r['Jam'] != 24 else 0}_D.png\")\n",
    "                ), axis=1).tolist()\n",
    "                # Filter hanya yang path-nya benar-benar ada\n",
    "                create_dataset.fallback_precursor_paths = [\n",
    "                    (z, h, d) for z, h, d in all_original_precursor_paths\n",
    "                    if os.path.exists(z) and os.path.exists(h) and os.path.exists(d)\n",
    "                ]\n",
    "                if not create_dataset.fallback_precursor_paths:\n",
    "                    raise ValueError(\"Tidak ditemukan gambar prekursor asli yang valid untuk fallback SMOTE!\")\n",
    "            \n",
    "            # Pilih salah satu path prekursor asli secara acak sebagai fallback untuk sampel SMOTE ini\n",
    "            rand_paths = create_dataset.fallback_precursor_paths[np.random.randint(len(create_dataset.fallback_precursor_paths))]\n",
    "            row['path_Z'], row['path_H'], row['path_D'] = rand_paths\n",
    "            valid_rows.append(row)\n",
    "            \n",
    "        # Jika itu bukan prekursor atau prekursor tapi tidak ada gambar dan bukan sintetik (e.g. data normal bermasalah), abaikan\n",
    "    except Exception as e:\n",
    "        # print(f\"Peringatan: Gagal memproses baris {index} - {e}. Mungkin file tidak ada atau metadata sintetik bermasalah.\")\n",
    "        continue # Lanjutkan ke baris berikutnya jika ada eror pada baris ini\n",
    "\n",
    "\n",
    "df_cleaned_combined = pd.DataFrame(valid_rows)\n",
    "print(f\"Validasi selesai. {len(df_full_combined) - len(df_cleaned_combined)} baris data dibuang. Total data valid: {len(df_cleaned_combined)}\")\n",
    "\n",
    "\n",
    "# --- STRATEGI PEMBAGIAN DATA ---\n",
    "# Kita ingin memastikan data sintetik hanya masuk ke TRAINING set untuk menghindari data leakage.\n",
    "# Kita akan memisahkan data asli dan data sintetik terlebih dahulu.\n",
    "\n",
    "df_real_data = df_cleaned_combined[df_cleaned_combined['label'].isin([0, 1]) & (df_cleaned_combined.index.isin(df_precursor_original.index) | df_cleaned_combined.index.isin(df_normal_original.index))]\n",
    "df_synthetic_data_for_training = df_cleaned_combined[~df_cleaned_combined.index.isin(df_real_data.index)]\n",
    "\n",
    "print(f\"Jumlah data asli yang valid: {len(df_real_data)}\")\n",
    "print(f\"Jumlah data sintetik SMOTE yang valid: {len(df_synthetic_data_for_training)}\")\n",
    "\n",
    "\n",
    "# Bagi data ASLI saja ke training, validasi, dan testing\n",
    "train_real_df, test_df = train_test_split(df_real_data, test_size=0.2, random_state=42, stratify=df_real_data['label'])\n",
    "train_real_df, val_df = train_test_split(train_real_df, test_size=0.15, random_state=42, stratify=train_real_df['label'])\n",
    "\n",
    "# Gabungkan data sintetik HANYA ke training set\n",
    "train_df = pd.concat([train_real_df, df_synthetic_data_for_training], ignore_index=True)\n",
    "\n",
    "print(\"\\n--- Ukuran Dataset Final (Asli + Sintetik SMOTE) ---\")\n",
    "print(f\"Jumlah data training (Asli + Sintetik) : {len(train_df)}\")\n",
    "print(f\"Jumlah data validasi (Hanya Asli)      : {len(val_df)}\")\n",
    "print(f\"Jumlah data testing (Hanya Asli)       : {len(test_df)}\")\n",
    "\n",
    "print(\"\\n--- Proporsi Kelas dalam Dataset Final ---\")\n",
    "print(\"Training (Asli + Sintetik):\")\n",
    "print(train_df['label'].value_counts(normalize=True))\n",
    "print(\"Validasi (Hanya Asli):\")\n",
    "print(val_df['label'].value_counts(normalize=True))\n",
    "print(\"Testing (Hanya Asli):\")\n",
    "print(test_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89035935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipa data (tf.data.Dataset) telah siap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761284776.667704  180336 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1761284776.667868  180336 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1761284776.784077  180336 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1761284776.784277  180336 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1761284776.805710  180336 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1560 MB memory:  -> device: 0, name: Quadro K620, pci bus id: 0000:02:00.0, compute capability: 5.0\n",
      "I0000 00:00:1761284776.806247  180336 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1737 MB memory:  -> device: 1, name: Quadro K620, pci bus id: 0000:03:00.0, compute capability: 5.0\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_image(path_z, path_h, path_d, label):\n",
    "    img_z = tf.io.read_file(path_z); img_z = tf.io.decode_png(img_z, channels=1)\n",
    "    img_h = tf.io.read_file(path_h); img_h = tf.io.decode_png(img_h, channels=1)\n",
    "    img_d = tf.io.read_file(path_d); img_d = tf.io.decode_png(img_d, channels=1)\n",
    "    image = tf.concat([img_z, img_h, img_d], axis=-1)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(df):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        df['path_Z'].values, df['path_H'].values, df['path_D'].values, \n",
    "        df['label'].values.astype(np.float32)\n",
    "    ))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=len(df)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df)\n",
    "test_dataset = create_dataset(test_df)\n",
    "\n",
    "print(\"\\nPipa data (tf.data.Dataset) telah siap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08c61bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Membangun Arsitektur Model Transfer Learning VGG16 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 13:49:40.056818: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_NO_BINARY_FOR_GPU'\n",
      "\n",
      "2025-10-24 13:49:40.056846: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-10-24 13:49:40.056858: W tensorflow/core/framework/op_kernel.cc:1842] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-10-24 13:49:40.056870: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- 1. Bangun Arsitektur ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Membangun Arsitektur Model Transfer Learning VGG16 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Diubah\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplications\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVGG16\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Diubah\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(IMG_SIZE, IMG_SIZE, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m~/Earthquake_Prediction/.conda/lib/python3.12/site-packages/keras/src/applications/vgg16.py:129\u001b[0m, in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[0m\n\u001b[1;32m    127\u001b[0m         img_input \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Block 1\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock1_conv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock1_conv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m )(x)\n\u001b[1;32m    135\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock1_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n",
      "File \u001b[0;32m~/Earthquake_Prediction/.conda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Earthquake_Prediction/.conda/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py:152\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse, ragged)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_int_dtype(dtype):\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;66;03m# TensorFlow conversion is stricter than other backends, it does not\u001b[39;00m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;66;03m# allow ints for bools or floats for ints. We convert without dtype\u001b[39;00m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# and cast instead.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m standardize_dtype(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m dtype:\n",
      "\u001b[0;31mInternalError\u001b[0m: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name: "
     ]
    }
   ],
   "source": [
    "# --- 1. Bangun Arsitektur ---\n",
    "print(\"--- Membangun Arsitektur Model Transfer Learning VGG16 ---\") # Diubah\n",
    "base_model = tf.keras.applications.VGG16( # Diubah\n",
    "    include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# Menggunakan preprocessing VGG16\n",
    "x = tf.keras.applications.vgg16.preprocess_input(inputs) # Diubah\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Gunakan EarlyStopping yang lebih agresif karena data training lebih sedikit\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history_head = model.fit(\n",
    "    train_dataset, epochs=25, validation_data=val_dataset,\n",
    "    callbacks=[early_stopping_callback]) \n",
    "\n",
    "# --- 3. Tahap 2: Fine-Tuning ---\n",
    "print(\"\\n--- Tahap 2: Melakukan Fine-Tuning ---\")\n",
    "base_model.trainable = True\n",
    "# Kita bisa unfreeze lebih banyak layer karena data training (termasuk sintetik) lebih banyak dan bervariasi\n",
    "for layer in base_model.layers[:-30]: # Unfreeze 30 layer terakhir (bisa disesuaikan)\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6), # Learning rate lebih rendah lagi\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_fine_tune = model.fit(\n",
    "    train_dataset, epochs=40, validation_data=val_dataset, # Lebih banyak epoch untuk fine-tuning\n",
    "    initial_epoch=len(history_head.epoch),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor=\"val_accuracy\", mode='max'),\n",
    "        early_stopping_callback])\n",
    "\n",
    "print(f\"--- Pelatihan Selesai. Model terbaik disimpan di {MODEL_SAVE_PATH} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SEL 6: LAPORAN CEPAT DAN EVALUASI AKHIR\n",
    "# ==============================================================================\n",
    "\n",
    "# Pastikan semua library yang dibutuhkan untuk plotting sudah diimpor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "\n",
    "# Bungkus semua dalam blok try-except untuk menangani NameError jika sel dijalankan tidak berurutan\n",
    "try:\n",
    "    # Cek apakah variabel riwayat pelatihan ada. Jika tidak, akan raise NameError.\n",
    "    if 'history_head' not in locals() and 'history_fine_tune' not in locals():\n",
    "        raise NameError(\"Variabel riwayat pelatihan (history_head/fine_tune) tidak ditemukan. Jalankan sel pelatihan (Sel 5) terlebih dahulu.\")\n",
    "    \n",
    "    print(f\"\\n{'='*25} LAPORAN HASIL PEMODELAN {'='*25}\")\n",
    "\n",
    "    # Muat model terbaik yang disimpan selama pelatihan\n",
    "    print(f\"Memuat model terbaik dari: {MODEL_SAVE_PATH}\")\n",
    "    best_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # BAGIAN 1: GRAFIK TRAINING DAN VALIDASI (AKURASI DAN LOSS)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- 1. Grafik Training dan Validasi (Akurasi dan Loss) ---\")\n",
    "\n",
    "    # Inisialisasi combined_history dengan list kosong\n",
    "    combined_history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
    "\n",
    "    # Fungsi pembantu untuk menambahkan riwayat ke combined_history dengan aman\n",
    "    def add_to_combined_history(source_history, target_history):\n",
    "        if source_history is not None:\n",
    "            for metric in ['accuracy', 'val_accuracy', 'loss', 'val_loss']:\n",
    "                if metric in source_history.history:\n",
    "                    target_history[metric].extend(source_history.history[metric])\n",
    "                else:\n",
    "                    print(f\"Peringatan: Metrik '{metric}' tidak ditemukan di history ini.\")\n",
    "\n",
    "    # Tambahkan history dari tahap head (jika ada)\n",
    "    if 'history_head' in locals():\n",
    "        add_to_combined_history(history_head, combined_history)\n",
    "    \n",
    "    # Tambahkan history dari tahap fine_tune (jika ada)\n",
    "    if 'history_fine_tune' in locals():\n",
    "        add_to_combined_history(history_fine_tune, combined_history)\n",
    "\n",
    "    def plot_training_history(history_dict):\n",
    "        \"\"\"Fungsi untuk membuat plot akurasi dan loss dari dictionary history.\"\"\"\n",
    "        acc = history_dict.get('accuracy')\n",
    "        val_acc = history_dict.get('val_accuracy')\n",
    "        loss = history_dict.get('loss')\n",
    "        val_loss = history_dict.get('val_loss')\n",
    "        \n",
    "        if not acc or not val_acc or not loss or not val_loss:\n",
    "            print(\"Tidak ada data lengkap untuk diplot (akurasi/loss mungkin kosong).\")\n",
    "            print(\"Periksa kembali riwayat pelatihan.\")\n",
    "            return\n",
    "\n",
    "        epochs = range(1, len(acc) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(16, 6))\n",
    "        \n",
    "        # Plot Akurasi\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, acc, 'bo-', label='Akurasi Training')\n",
    "        plt.plot(epochs, val_acc, 'ro-', label='Akurasi Validasi')\n",
    "        plt.title('Grafik Akurasi Model', fontsize=16)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Akurasi')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, loss, 'bo-', label='Loss Training')\n",
    "        plt.plot(epochs, val_loss, 'ro-', label='Loss Validasi')\n",
    "        plt.title('Grafik Loss Model', fontsize=16)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (Binary Crossentropy)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Panggil fungsi plotting\n",
    "    plot_training_history(combined_history)\n",
    "    print(\"Berikut adalah grafik yang menampilkan Akurasi dan Loss selama proses training dan validasi.\")\n",
    "    print(\"Garis 'bo-' mewakili data training dan garis 'ro-' mewakili data validasi.\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # BAGIAN 2: ANALISIS DETAIL KINERJA PADA DATA TESTING\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- 2. Analisis Detail Kinerja pada Data Test ---\")\n",
    "    \n",
    "    # Kumpulkan semua gambar dan label dari test_dataset ke dalam list\n",
    "    all_test_images = []\n",
    "    all_test_labels = []\n",
    "    for images, labels in test_dataset:\n",
    "        all_test_images.append(images)\n",
    "        all_test_labels.append(labels)\n",
    "    \n",
    "    y_true = np.array([])\n",
    "    y_pred_probs = np.array([])\n",
    "\n",
    "    if all_test_images: # Pastikan ada data test\n",
    "        test_images_tensor = tf.concat(all_test_images, axis=0)\n",
    "        test_labels_tensor = tf.concat(all_test_labels, axis=0)\n",
    "        \n",
    "        y_true = test_labels_tensor.numpy()\n",
    "        print(f\"Membuat prediksi pada {len(y_true)} sampel data test...\")\n",
    "        y_pred_probs = best_model.predict(test_images_tensor, verbose=1).flatten()\n",
    "    else:\n",
    "        print(\"Peringatan: test_dataset kosong. Tidak dapat melakukan evaluasi.\")\n",
    "\n",
    "\n",
    "    if len(y_true) > 0: # Lanjutkan hanya jika ada data test yang valid\n",
    "        # --- DIAGNOSA ---\n",
    "        print(\"\\n--- DIAGNOSA: Cek y_true dan y_pred_probs ---\")\n",
    "        print(f\"Bentuk y_true: {y_true.shape}\")\n",
    "        print(f\"Bentuk y_pred_probs: {y_pred_probs.shape}\")\n",
    "        print(f\"Nilai unik di y_true: {np.unique(y_true)}\")\n",
    "        print(f\"Jumlah sampel y_true Kelas 0: {np.sum(y_true == 0)}\")\n",
    "        print(f\"Jumlah sampel y_true Kelas 1: {np.sum(y_true == 1)}\")\n",
    "        \n",
    "        print(\"\\n--- Distribusi Probabilitas Prediksi Kelas 1 (Prekursor) ---\")\n",
    "        print(f\"Min prob: {y_pred_probs.min():.4f}\")\n",
    "        print(f\"Max prob: {y_pred_probs.max():.4f}\")\n",
    "        print(f\"Mean prob: {y_pred_probs.mean():.4f}\")\n",
    "        print(f\"Median prob: {np.median(y_pred_probs):.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(y_pred_probs, bins=50, kde=True, color='skyblue')\n",
    "        plt.title('Distribusi Probabilitas Prediksi Kelas 1 (Prekursor)')\n",
    "        plt.xlabel('Probabilitas')\n",
    "        plt.ylabel('Frekuensi')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        num_above_0_1 = np.sum(y_pred_probs > 0.1)\n",
    "        print(f\"Jumlah prediksi probabilitas > 0.1: {num_above_0_1} dari {len(y_pred_probs)}\")\n",
    "        print(f\"Jumlah sampel aktual Kelas 1 (Prekursor) di data test: {np.sum(y_true == 1)}\")\n",
    "        \n",
    "        # Konversi probabilitas (0-1) menjadi kelas (0 atau 1) dengan ambang batas 0.5\n",
    "        y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "        \n",
    "        print(f\"Nilai unik di y_pred (threshold 0.5): {np.unique(y_pred)}\")\n",
    "        print(f\"Jumlah prediksi y_pred Kelas 0 (threshold 0.5): {np.sum(y_pred == 0)}\")\n",
    "        print(f\"Jumlah prediksi y_pred Kelas 1 (threshold 0.5): {np.sum(y_pred == 1)}\")\n",
    "\n",
    "\n",
    "        # --- A. Tabel Laporan Klasifikasi ---\n",
    "        print(\"\\n a. Tabel Laporan Klasifikasi (Precision, Recall, F1-Score):\")\n",
    "        print(classification_report(y_true, y_pred, target_names=['Normal (Kelas 0)', 'Prekursor (Kelas 1)'], zero_division=0))\n",
    "\n",
    "        # --- B. Grafik Confusion Matrix ---\n",
    "        print(\"\\n b. Grafik Confusion Matrix:\")\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_normalized = np.zeros_like(cm, dtype=float)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            cm_normalized[np.isnan(cm_normalized)] = 0 \n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                    xticklabels=['Prediksi Normal', 'Prediksi Prekursor'],\n",
    "                    yticklabels=['Aktual Normal', 'Aktual Prekursor'])\n",
    "        plt.title('Confusion Matrix (Ternormalisasi)', fontsize=16)\n",
    "        plt.ylabel('Kelas Aktual (Sebenarnya)')\n",
    "        plt.xlabel('Kelas Prediksi (Tebakan Model)')\n",
    "        plt.show()\n",
    "\n",
    "        # --- C. Grafik Kurva ROC & AUC ---\n",
    "        print(\"\\n c. Grafik Kurva ROC dan Nilai AUC:\")\n",
    "        auc_score = roc_auc_score(y_true, y_pred_probs)\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Kurva ROC (AUC = {auc_score:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Tebakan Acak (AUC = 0.5)')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate (Rasio Alarm Palsu)')\n",
    "        plt.ylabel('True Positive Rate (Recall / Sensitivitas)')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nNilai Area Under Curve (AUC): {auc_score:.4f}\")\n",
    "        print(\"Semakin nilai AUC mendekati 1.0, semakin baik kemampuan model dalam membedakan kedua kelas.\")\n",
    "        \n",
    "        # --- D. Analisis Ambang Batas Klasifikasi ---\n",
    "        print(\"\\n d. Analisis Kinerja pada Berbagai Ambang Batas Klasifikasi:\")\n",
    "\n",
    "        thresholds_range = np.arange(0.1, 1.0, 0.05) \n",
    "        \n",
    "        best_f1_prec = -1 \n",
    "        best_threshold = 0.5\n",
    "\n",
    "        performance_data = []\n",
    "\n",
    "        print(\"{:<10} {:<10} {:<10} {:<10} {:<10}\".format(\"Threshold\", \"P_Prec\", \"R_Prec\", \"F1_Prec\", \"Accuracy\"))\n",
    "\n",
    "        for threshold in thresholds_range:\n",
    "            y_pred_tuned = (y_pred_probs > threshold).astype(int)\n",
    "            \n",
    "            try:\n",
    "                unique_preds = np.unique(y_pred_tuned)\n",
    "                if len(unique_preds) == 1:\n",
    "                    if unique_preds[0] == 0: \n",
    "                        p_prec = 0.0\n",
    "                        r_prec = 0.0\n",
    "                        f1_prec = 0.0\n",
    "                        accuracy_tuned = np.mean(y_true == y_pred_tuned)\n",
    "                    else: \n",
    "                        tp = np.sum((y_true == 1) & (y_pred_tuned == 1))\n",
    "                        fp = np.sum((y_true == 0) & (y_pred_tuned == 1))\n",
    "                        fn = np.sum((y_true == 1) & (y_pred_tuned == 0))\n",
    "                        \n",
    "                        p_prec = tp / (tp + fp + 1e-10) \n",
    "                        r_prec = tp / (tp + fn + 1e-10)\n",
    "                        f1_prec = 2 * (p_prec * r_prec) / (p_prec + r_prec + 1e-10) if (p_prec + r_prec) > 0 else 0.0\n",
    "                        accuracy_tuned = np.mean(y_true == y_pred_tuned)\n",
    "                else: \n",
    "                    cr = classification_report(y_true, y_pred_tuned, output_dict=True, zero_division=0, labels=[0, 1])\n",
    "                    p_prec = cr['1']['precision'] \n",
    "                    r_prec = cr['1']['recall']    \n",
    "                    f1_prec = cr['1']['f1-score'] \n",
    "                    accuracy_tuned = cr['accuracy']\n",
    "            except KeyError: \n",
    "                p_prec = 0.0\n",
    "                r_prec = 0.0\n",
    "                f1_prec = 0.0\n",
    "                accuracy_tuned = np.mean(y_true == y_pred_tuned) \n",
    "\n",
    "            performance_data.append({\n",
    "                'Threshold': threshold,\n",
    "                'P_Prec': p_prec, \n",
    "                'R_Prec': r_prec, \n",
    "                'F1_Prec': f1_prec, \n",
    "                'Accuracy': accuracy_tuned\n",
    "            })\n",
    "\n",
    "            print(\"{:<10.2f} {:<10.2f} {:<10.2f} {:<10.2f} {:<10.2f}\".format(threshold, p_prec, r_prec, f1_prec, accuracy_tuned))\n",
    "\n",
    "            if f1_prec > best_f1_prec: \n",
    "                best_f1_prec = f1_prec \n",
    "                best_threshold = threshold\n",
    "\n",
    "        print(f\"\\nAmbang batas terbaik berdasarkan F1-score Prekursor: {best_threshold:.2f} (F1 = {best_f1_prec:.2f})\")\n",
    "        \n",
    "        # Visualisasi Precision dan Recall vs. Threshold\n",
    "        df_performance = pd.DataFrame(performance_data)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df_performance['Threshold'], df_performance['P_Prec'], label='Precision Prekursor', marker='o')\n",
    "        plt.plot(df_performance['Threshold'], df_performance['R_Prec'], label='Recall Prekursor', marker='o') \n",
    "        plt.plot(df_performance['Threshold'], df_performance['F1_Prec'], label='F1-Score Prekursor', marker='o', linestyle='--')\n",
    "        plt.title('Precision, Recall, dan F1-Score Prekursor vs. Threshold', fontsize=16)\n",
    "        plt.xlabel('Ambang Batas (Threshold)')\n",
    "        plt.ylabel('Skor')\n",
    "        plt.axvline(x=0.5, color='gray', linestyle=':', label='Threshold Default (0.5)')\n",
    "        plt.axvline(x=best_threshold, color='green', linestyle='--', label=f'Best F1 Threshold ({best_threshold:.2f})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"\\nTidak ada data test yang cukup untuk melakukan evaluasi detail.\")\n",
    "\n",
    "    print(f\"\\n{'='*25} LAPORAN SELESAI {'='*25}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\n❌ EROR: Variabel yang dibutuhkan tidak ditemukan ({e}).\")\n",
    "    print(\"Pastikan Anda telah menjalankan sel pelatihan (Sel 5) hingga selesai pada sesi ini.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ EROR: File model tidak ditemukan ({e}).\")\n",
    "    print(f\"Pastikan model telah disimpan dengan benar di {MODEL_SAVE_PATH} setelah pelatihan.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Terjadi kesalahan tak terduga: {e}\")\n",
    "    print(\"Periksa kembali pesan eror di atas untuk detail lebih lanjut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936c0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
